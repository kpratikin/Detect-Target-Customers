---
title: "Best Buy Case Study"
author: ""
output: word_document
---
#==========================================================
## SET UP R MARKDOWN
#==========================================================
```{r}
# You should generally clear the working space at the start of every R session
rm(list = ls())

#setwd("C:/Users/nertekin/Dropbox/Santa Clara Teaching/OMIS 2392/RProjects/Week-2")
setwd("E:/SCU Business Analytics/Econometric in R/Case Studies/Case Study 3/")

# install packages
#install.packages("ggeffects")
#install.packages("QuantPsyc")
#install.packages("VIF")
#install.packages("usdm")
#install.packages("lmtest")
#install.packages("multiwayvcov")
#install.packages("sandwich")
#install.packages("AER")
#install.packages("aod")
#install.packages("mfx")

# Load libraries everytime you start a session
library(stargazer)
library(gdata)
library(ggplot2)
#library(psych) 
library(ggeffects)
library(QuantPsyc)
library(usdm)
library(lmtest)
library(multiwayvcov)
library(sandwich)
library(foreign)
library(AER)
library(aod)
library(Rcpp)
library(mfx)
library(nnet)
library(reshape2)


# turn off scientific notation except for big numbers. 
options(scipen = 9)
```
#==========================================================
##  LOAD AND EXPLORE DATA
#==========================================================
```{r}
mydata = read.csv("BestBuy.csv")

# Summary statistics
stargazer(mydata, type="text", median=TRUE, iqr=TRUE,digits=4, title="Descriptive Statistics")  

hist(mydata$hhincome)
hist(log(mydata$hhincome)) #log of hhincome is more normally distributed compared to hhincome

hist(mydata$PriceCategory) 
hist(log(mydata$PriceCategory)) # PriceCategory is more normally distributed as compared to log

#Create box plots from Lab Session 1.rmd

## Detecting Multicollinearity
df=data.frame(mydata$age,mydata$hisp,mydata$PriceCategory,mydata$married,mydata$MyBestBuy,mydata$hhincome, mydata$appliances,mydata$familysize,mydata$productgeneration, mydata$newcustomer, mydata$weekend)
cor(df) #collinearity > 0.8 between "Married and Family Size" and "Price Category and Product Generation" 
vif(df)
vifcor(df) #VIF score greater than 3 for "Married and Family Size" and "Price Category has high collinearity with Product Generation" 

df2=data.frame(mydata$age,mydata$hisp,mydata$PriceCategory,mydata$married,mydata$MyBestBuy,mydata$hhincome, mydata$appliances,mydata$familysize, mydata$newcustomer, mydata$weekend) #Removing product generation
vifcor(df2)

df3=data.frame(mydata$age,mydata$hisp,mydata$PriceCategory,mydata$MyBestBuy,mydata$hhincome, mydata$appliances,mydata$familysize, mydata$newcustomer, mydata$weekend) #Removing married
vifcor(df3)

```

#==========================================================
## MODEL DEVELOPMENT
#==========================================================

```{r}
#Since the dependent variable is binary, logit and probit model will be used but running the OLS model for reference.

res = lm(Warranty~PriceCategory+appliances+familysize+log(hhincome+1)+hisp+newcustomer+MyBestBuy+age+weekend,data=mydata)

#Interaction between Price Category and Appliance 
res2 = lm(Warranty~PriceCategory+appliances+PriceCategory*appliances+familysize+log(hhincome+1)+hisp+newcustomer+MyBestBuy+age+weekend,data=mydata) 


stargazer(res,res2,
          title="Regression Results", type="text", 
          column.labels=c("Simple","Interaction"),
          df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001))

mydata$predictedprobability_lm<-predict(res2) # let's look at the predicted probability of return for each observation in the data 
ggplot(mydata, aes(y=predictedprobability_lm, x=PriceCategory)) + geom_point(size=2.5)
range(mydata$predictedprobability_lm) # Range of the predicted probability tells us there are "negative" probabilities of return for some observations!!! This cannot be possible. Therefore, linear probability model is not the right model  

```

#==========================================================
## LOGIT
#==========================================================

```{r}
sum(mydata$Warranty==0)
sum(mydata$Warranty==1) # We have 1216 observations with Warranty=0 and 1990 observations with Warranty=1. Considering that we will estimate 11 parameters(Family Size(Factor=4)+Price Category and Appliance(Interaction=1)+Other variables(6)), we satisfy the minimum 20:1 ratio requirement

#Interaction between Price Category and Appliance
logit1<- glm(Warranty~PriceCategory+appliances+PriceCategory*appliances+familysize+log(hhincome+1)+hisp+newcustomer+MyBestBuy+age+weekend,data=mydata, family="binomial") 

stargazer(logit1,
          title="Regression Results", type="text", 
          column.labels=c("Logit-1"),
          df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001)) # For every one unit change in price, the log odds of return (vs. no return) increases by 0.002. For every one unit change in selling pressure, the log odds of return (vs. no return) increases by 0.8. Being in a mall with grade 2 versus in a mall with grade 1 changes the log odds of return by -0.68. 

stargazer(logit1, 
          apply.coef = exp, t.auto=F, p.auto = F,
          title="Regression Results", type="text", 
          column.labels=c("OddsRatios"),
          df=FALSE, digits=4, star.cutoffs = c(0.05,0.01,0.001)) # Let's obtain odds ratios. Now we can say that for a one unit increase in selling pressure, the odds of being returned (versus not being returned) increase by a factor of 2.23. Note that while R produces it, the odds ratio for the intercept is not generally interpreted. 


## Model fit assessment 
logit1a <- glm(Warranty~1, data=mydata, family="binomial") # This is the command to run a logit on null model 
lrtest(logit1, logit1a) #We compare the null model to our model to determine the model fit. The chi-square of 346.67 with 11 degrees of freedom and an associated p-value of less than 0.001 tells us that our model as a whole fits significantly better than the null model.


## Measuring the predictive power of the logit
pred = predict(logit1, data=mydata,type = "response") # Let's generate predicted probabilities
return_prediction <- ifelse(pred >= 0.5,1,0) # If the predicted probability is greater than 0.5, then the predicted classification will be a warranty (warranty==1), otherwise it will be a no warranty (warranty==0)
misClasificError <- mean(return_prediction != mydata$Warranty) # count number of wrong classifications
print(paste('Accuracy',1-misClasificError)) # calculate the correct classification rate. Accuracy is 0.6715, meaning the model correctly determines the membership (being 0 vs 1) for 67.15% of all observations
table(mydata$Warranty, pred>=0.5) # This generates the confusion matrix


# Check for heteroscedasticity
gqtest(logit1) # Significant Goldfeld-Quandt test does not indicate heteroscedasticity 
bptest(logit1) # Significant Breusch-Pagan test indicates heteroscedasticity

HWrobstder <- sqrt(diag(vcovHC(logit1, type="HC1"))) # produces Huber-White robust standard errors 

stargazer(logit1, logit1,  
          se=list(NULL, HWrobstder),
          title="Regression Results", type="text", 
          column.labels=c("Normal SE", "HW-Robust SE"),
          df=FALSE, digits=3, star.cutoffs = c(0.05,0.01,0.001))  # displays normal/HW robust  standard errors. objective quality is not significant and perceived quality is significant and as expected

## Obtain marginal effects
a <- logitmfx(formula=Warranty~PriceCategory+appliances+PriceCategory*appliances+familysize+log(hhincome+1)+hisp+newcustomer+MyBestBuy+age+weekend, data=mydata) # We can generate the marginal effects with this command. The one unit increase in selling pressure increases the probability of return by 0.168, holding other variables at their means
marginaleffects <- a$mfxest[,1]
marg.std.err <- a$mfxest[,2]

stargazer(logit1,
          omit=c("Constant"),
          coef = list(marginaleffects), se = list(marg.std.err),
          title="Regression Results", type="text", 
          column.labels=c("Marginal Effects"),
          df=FALSE, digits=4, star.cutoffs = c(0.05,0.01,0.001))

b <- logitmfx(formula=Warranty~PriceCategory+appliances+PriceCategory*appliances+familysize+log(hhincome+1)+hisp+newcustomer+MyBestBuy+age+weekend, data=mydata, robust=TRUE) # We can obtain the marginal effects from a logit that uses robust standard errors. Note that marginal effects do not change, however, std. errors, and therefore, p-values change.
rob.std.err <- b$mfxest[,2]

stargazer(logit1, logit1,
          se=list(marg.std.err, rob.std.err),
          omit=c("Constant"),
          coef = list(marginaleffects,marginaleffects),
          title="Regression Results", type="text", 
          column.labels=c("Marginal Effects","Marg.Eff.w/RobStdEr" ),
          df=FALSE, digits=4, star.cutoffs = c(0.05,0.01,0.001))


#FAMILY SIZE AS FACTOR VARIABLE

mydata$familyfactor <- factor(mydata$familysize) 
table(mydata$familysize)
mydata$familyfactor <- factor(mydata$familyfactor, levels = c(1,2,3,4)) 


logit2<- glm(Warranty~PriceCategory+appliances+PriceCategory*appliances+factor(mydata$familysize)+log(hhincome+1)+hisp+newcustomer+MyBestBuy+age+weekend, data=mydata, family="binomial") 


stargazer(logit2,
          title="Regression Results", type="text", 
          column.labels=c("Logit-1"),
          df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001)) # For every one unit change in price, the log odds of return (vs. no return) increases by 0.002. For every one unit change in selling pressure, the log odds of return (vs. no return) increases by 0.8. Being in a mall with grade 2 versus in a mall with grade 1 changes the log odds of return by -0.68. 

stargazer(logit2, 
          apply.coef = exp, t.auto=F, p.auto = F,
          title="Regression Results", type="text", 
          column.labels=c("OddsRatios"),
          df=FALSE, digits=4, star.cutoffs = c(0.05,0.01,0.001)) # Let's obtain odds ratios. Now we can say that for a one unit increase in selling pressure, the odds of being returned (versus not being returned) increase by a factor of 2.23. Note that while R produces it, the odds ratio for the intercept is not generally interpreted. 

## Model fit assessment 
logit2a <- glm(Warranty~1, data=mydata, family="binomial") # This is the command to run a logit on null model 
lrtest(logit2, logit2a) #We compare the null model to our model to determine the model fit. The chi-square of 366.37 with 13 degrees of freedom and an associated p-value of less than 0.001 tells us that our model as a whole fits significantly better than the null model.

## Choosing a model between two alternatives 
stargazer(logit1, logit2,
          title="Regression Results", type="text", 
          column.labels=c("Logit-1", "Logit-2"),
          df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001))

lrtest(logit1, logit2) # The p-value of 5.281e-05 indicates that the making family size as factor variable improves the model fit, therefore we choose the model with family size as factor variable
anova(logit1, logit2, test="Chisq") # the same exact test can be conducted using anova as we learned before

# Check for heteroscedasticity
gqtest(logit2) # Significant Goldfeld-Quandt test does not indicate heteroscedasticity 
bptest(logit2) # Significant Breusch-Pagan test indicates heteroscedasticity
HWrobstder2 <- sqrt(diag(vcovHC(logit2, type="HC1"))) # produces Huber-White robust standard errors 

## Obtain marginal effects
a <- logitmfx(formula=Warranty~PriceCategory+appliances+PriceCategory*appliances+factor(mydata$familysize)+log(hhincome+1)+hisp+newcustomer+MyBestBuy+age+weekend, data=mydata) # We can generate the marginal effects with this command. The one unit increase in selling pressure increases the probability of return by 0.168, holding other variables at their means
marginaleffects2 <- a$mfxest[,1]
marg.std.err2 <- a$mfxest[,2]

stargazer(logit2,
          omit=c("Constant"),
          coef = list(marginaleffects2), se = list(marg.std.err2),
          title="Regression Results", type="text", 
          column.labels=c("Marginal Effects"),
          df=FALSE, digits=4, star.cutoffs = c(0.05,0.01,0.001))

b <- logitmfx(formula=Warranty~PriceCategory+appliances+PriceCategory*appliances+factor(mydata$familysize)+log(hhincome+1)+hisp+newcustomer+MyBestBuy+age+weekend, data=mydata, robust=TRUE) # We can obtain the marginal effects from a logit that uses robust standard errors. Note that marginal effects do not change, however, std. errors, and therefore, p-values change.
rob.std.err2 <- b$mfxest[,2]

stargazer(logit2, logit2,
          se=list(marg.std.err2, rob.std.err2),
          omit=c("Constant"),
          coef = list(marginaleffects2,marginaleffects2),
          title="Regression Results", type="text", 
          column.labels=c("Marginal Effects","Marg.Eff.w/RobStdEr" ),
          df=FALSE, digits=4, star.cutoffs = c(0.05,0.01,0.001))


## Measuring the predictive power of the logit
pred = predict(logit2, data=mydata,type = "response") # Let's generate predicted probabilities
return_prediction <- ifelse(pred >= 0.5,1,0) # If the predicted probability is greater than 0.5, then the predicted classification will be a return (return==1), otherwise it will be a no return (return==0W 
misClasificError <- mean(return_prediction != mydata$Warranty) # count number of wrong classifications
print(paste('Accuracy',1-misClasificError)) # calculate the correct classification rate. Accuracy is 0.6806, meaning the model correctly determines the membership (being 0 vs 1) for 68.06% of all observations
table(mydata$Warranty, pred>=0.5) # This generates the confusion matrix



#INTERACTION BETWEEN Family Size and hhincome

logit3<- glm(Warranty~PriceCategory+appliances+PriceCategory*appliances+factor(mydata$familysize)+log(hhincome+1)+hisp+newcustomer+MyBestBuy+age+weekend+log(hhincome+1)*familysize, data=mydata, family="binomial") # This is the command to run a logit regression 

stargazer(logit3,
          title="Regression Results", type="text", 
          column.labels=c("Logit-1"),
          df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001)) # For every one unit change in price, the log odds of return (vs. no return) increases by 0.002. For every one unit change in selling pressure, the log odds of return (vs. no return) increases by 0.8. Being in a mall with grade 2 versus in a mall with grade 1 changes the log odds of return by -0.68. 

stargazer(logit3, 
          apply.coef = exp, t.auto=F, p.auto = F,
          title="Regression Results", type="text", 
          column.labels=c("OddsRatios"),
          df=FALSE, digits=4, star.cutoffs = c(0.05,0.01,0.001)) # Let's obtain odds ratios. Now we can say that for a one unit increase in selling pressure, the odds of being returned (versus not being returned) increase by a factor of 2.23. Note that while R produces it, the odds ratio for the intercept is not generally interpreted. 

## Model fit assessment 
logit3a <- glm(Warranty~1, data=mydata, family="binomial") # This is the command to run a logit on null model 
lrtest(logit3, logit3a) #We compare the null model to our model to determine the model fit. The chi-square of 41.46 with 5 degrees of freedom and an associated p-value of less than 0.001 tells us that our model as a whole fits significantly better than the null model.

## Choosing a model between two alternatives 
stargazer(logit2, logit3,
          title="Regression Results", type="text", 
          column.labels=c("Logit-1", "Logit-2"),
          df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001))

lrtest(logit2, logit3) # The p-value of  0.4934 indicates that taking the interaction between hhincome and family size does not improves the model fit, therefore we choose the model with family size as factor variable
anova(logit2, logit3, test="Chisq") # the same exact test can be conducted using anova as we learned before

## Measuring the predictive power of the logit
pred = predict(logit3, data=mydata,type = "response") # Let's generate predicted probabilities
return_prediction <- ifelse(pred >= 0.5,1,0) # If the predicted probability is greater than 0.5, then the predicted classification will be a return (return==1), otherwise it will be a no return (return==0W 
misClasificError <- mean(return_prediction != mydata$Warranty) # count number of wrong classifications
print(paste('Accuracy',1-misClasificError)) # calculate the correct classification rate. Accuracy is 0.69, meaning the model correctly determines the membership (being 0 vs 1) for 71% of all observations
table(mydata$Warranty, pred>=0.5) # This generates the confusion matrix



#HHINCOME AS QUADRATIC

ggplot(mydata, aes(x=hhincome, y=Warranty)) + geom_point(size=2.5) # the scatter plot indicates a quadratic relationship

logit4<- glm(Warranty~PriceCategory+appliances+PriceCategory*appliances+factor(mydata$familysize)+log(hhincome+1)+I(log(hhincome+1)^2)+hisp+newcustomer+MyBestBuy+age+weekend, data=mydata, family="binomial") # This is the command to run a logit regression 

stargazer(logit4,
          title="Regression Results", type="text", 
          column.labels=c("Logit-1"),
          df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001)) # For every one unit change in price, the log odds of return (vs. no return) increases by 0.002. For every one unit change in selling pressure, the log odds of return (vs. no return) increases by 0.8. Being in a mall with grade 2 versus in a mall with grade 1 changes the log odds of return by -0.68. 

stargazer(logit4, 
          apply.coef = exp, t.auto=F, p.auto = F,
          title="Regression Results", type="text", 
          column.labels=c("OddsRatios"),
          df=FALSE, digits=4, star.cutoffs = c(0.05,0.01,0.001)) # Let's obtain odds ratios. Now we can say that for a one unit increase in selling pressure, the odds of being returned (versus not being returned) increase by a factor of 2.23. Note that while R produces it, the odds ratio for the intercept is not generally interpreted. 

## Model fit assessment 
logit4a <- glm(Warranty~1, data=mydata, family="binomial") # This is the command to run a logit on null model 
lrtest(logit4, logit4a) #We compare the null model to our model to determine the model fit. The chi-square of 367.43 with 14 degrees of freedom and an associated p-value of less than 0.001 tells us that our model as a whole fits significantly better than the null model.


## Choosing a model between two alternatives 
stargazer(logit2, logit4,
          title="Regression Results", type="text", 
          column.labels=c("Logit-1", "Logit-2"),
          df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001))

lrtest(logit2, logit4) # The p-value of 5.281e-05 indicates that the making family size as factor variable improves the model fit, therefore we choose the model with family size as factor variable
anova(logit2, logit4, test="Chisq") # the same exact test can be conducted using anova as we learned before

## Measuring the predictive power of the logit
pred = predict(logit4, data=mydata,type = "response") # Let's generate predicted probabilities
return_prediction <- ifelse(pred >= 0.5,1,0) # If the predicted probability is greater than 0.5, then the predicted classification will be a return (return==1), otherwise it will be a no return (return==0W 
misClasificError <- mean(return_prediction != mydata$Warranty) # count number of wrong classifications
print(paste('Accuracy',1-misClasificError)) # calculate the correct classification rate. Accuracy is 0.6812, meaning the model correctly determines the membership (being 0 vs 1) for 68.12% of all observations
table(mydata$Warranty, pred>=0.5) # This generates the confusion matrix

```
```{r}

#MARGINAL EFFECTS FOR SIGNIFICANT VARIABLES
mydata$factorfamily <- as.factor(mydata$familysize)

mydata$logincome<-log(mydata$hhincome+1)

logitf<- glm(Warranty~PriceCategory+appliances+PriceCategory*appliances+newcustomer+MyBestBuy+age+factorfamily+logincome+hisp+weekend, data=mydata, family="binomial")

meffectsV <- ggpredict(logitf, terms=c("PriceCategory", "appliances")) # generates a tidy data frame  
ggplot(meffectsV,aes(x, predicted, colour=group)) + geom_line(size=1.3) + 
    xlab("Price Category") + ylab("Warranty Propensity") +
    labs(colour="Home\nAppliances?") + 
    scale_colour_discrete(labels=c("No", "Yes"))

dfset1<- subset(mydata, appliances==0)
dfset2<- subset(mydata, appliances==1)

ModelA = glm(Warranty~PriceCategory+appliances+PriceCategory*appliances+newcustomer+MyBestBuy+age+factorfamily+logincome+hisp+weekend, data=dfset1, family="binomial")

ModelB = glm(Warranty~PriceCategory+appliances+PriceCategory*appliances+newcustomer+MyBestBuy+age+factorfamily+logincome+hisp+weekend, data=dfset2, family="binomial")

stargazer(ModelA, ModelB,
          title="Regression Results", type="text", 
          column.labels=c("Non Home Appliance", "Home Appliance"),
          df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001))  



#FOR HISPANIC
meffectsV2 <- ggpredict(logitf, terms=c("hisp")) # generates a tidy data frame  

ggplot(meffectsV2,aes(x, predicted, colour=group)) + geom_line(size=1.3) + 
    xlab("Hispanic") + ylab("Warranty Propensity") 



#FOR HHINCOME
meffectsV3 <- ggpredict(logitf, terms=c("logincome"))
ggplot(meffectsV3,aes(x, predicted, colour=group)) + geom_line(size=1.3) + 
    xlab("LogHHIncome") + ylab("Warranty") 


#FOR FAMILYSIZE
meffectsV4 <- ggpredict(logitf, terms=c("factorfamily")) # generates a tidy data frame at three different values of age  
ggplot(meffectsV4,aes(x, predicted, colour=group)) + geom_line(size=1.3) + 
    xlab("FamilySize") + ylab("Warranty") 


```

#==========================================================
## PROBIT
#==========================================================

```{r}


probit1<- glm(Warranty~PriceCategory+appliances+PriceCategory*appliances+factor(mydata$familysize)+log(hhincome+1)+hisp+newcustomer+MyBestBuy+age+weekend, data=mydata, family=binomial(link="probit")) # This is the command to run a probit regression 
stargazer(probit1,
          title="Regression Results", type="text", 
          column.labels=c("Probit-1"),
          df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001)) # For every one unit change in price, the z-score increases by 0.001. For every one unit change in selling pressure, the z-score increases by 0.48. Being in a mall with grade 2 versus in a mall with grade 1 decreases the z-score by 0.42. 

## Model fit assessment 
probit1a <- glm(Warranty~1, data=mydata, family=binomial(link="probit")) # This is the command to run a logit on null model 
lrtest(probit1, probit1a) #We compare the null model to our model to determine the model fit. The chi-square of 41.56 with 5 degrees of freedom and an associated p-value of less than 0.001 tells us that our model as a whole fits significantly better than the null model.


## Obtain marginal effects
a <- probitmfx(formula=Warranty~PriceCategory+appliances+PriceCategory*appliances+newcustomer+MyBestBuy+age+familysize+hhincome+hisp+weekend, data=mydata) # We can generate the marginal effects with this command. The one unit increase in selling pressure increases the probability of return by 0.168, holding other variables at their means
marginaleffects_probit <- a$mfxest[,1]
marg.std.err_probit <- a$mfxest[,2]

stargazer(probit1, logit1, 
          omit=c("Constant"),
          coef = list(marginaleffects_probit,marginaleffects), se = list(marg.std.err_probit,marg.std.err),
          title="Marginal Effects", type="text", 
          column.labels=c("Probit", "Logit"),
          df=FALSE, digits=4, star.cutoffs = c(0.05,0.01,0.001))

b <- probitmfx(formula=Warranty~PriceCategory+appliances+PriceCategory*appliances+newcustomer+MyBestBuy+age+familysize+hhincome+hisp+weekend, data=mydata, robust=TRUE) # We can obtain the marginal effects from a probit that uses robust standard errors. Note that marginal effects do not change, however, std. errors, and therefore, p-values change.
rob.std.err <- b$mfxest[,2]

stargazer(probit1, probit1,
          se=list(marg.std.err, rob.std.err),
          omit=c("Constant"),
          coef = list(marginaleffects,marginaleffects),
          title="Regression Results", type="text", 
          column.labels=c("Marginal Effects","Marg.Eff.w/RobStdEr" ),
          df=FALSE, digits=4, star.cutoffs = c(0.05,0.01,0.001))


## Measuring the predictive power of the probit
pred = predict(probit1, data=mydata, type="response") # Let's generate predicted probabilities

return_prediction <- ifelse(pred >= 0.5,1,0) # If the predicted probability is greater than 0.5, then the predicted classification will be a return (return==1), otherwise it will be a no return (return==0) 
misClasificError <- mean(return_prediction != mydata$Warranty) # count number of wrong classifications
print(paste('Accuracy',1-misClasificError)) # calculate the correct classification rate. Accuracy is 0.6802, meaning the model correctly determines the membership (being 0 vs 1) for 68.02% of all observations
table(mydata$Warranty, pred>=0.5) # This generates the confusion matrix
```





